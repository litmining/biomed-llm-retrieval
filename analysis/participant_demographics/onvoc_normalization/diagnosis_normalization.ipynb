{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import List, Dict, Union\n",
    "from pathlib import Path\n",
    "from functools import partial \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = Path('/data/alejandro/projects/ns-pond/pipeline_outputs/participant_demographics/ParticipantDemographicsExtractor/1.0.0/fd0599e01921/')\n",
    "projects_folder = Path('/data/alejandro/projects/')\n",
    "\n",
    "results_df = pd.read_csv('pd_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(input_string: str) -> str:\n",
    "    \"\"\"Normalize a string by removing leading/trailing whitespace and converting to lowercase.\n",
    "    Args:\n",
    "        input_string (str): The string to normalize.\n",
    "    Returns:\n",
    "        str: The normalized string.\n",
    "    \"\"\"\n",
    "    clean_string = string.capwords(input_string.strip())\n",
    "    clean_string = clean_string.replace(\"â€™\", \"'\")\n",
    "    if clean_string == \"\":\n",
    "        return None\n",
    "    if clean_string == \"None\":\n",
    "        return None\n",
    "    if clean_string == \"Nan\":\n",
    "        return None\n",
    "    if clean_string == \"N/A\":\n",
    "        return None\n",
    "    if clean_string == \"Null\":\n",
    "        return None\n",
    "\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "def load_abbreviations(\n",
    "    text: str, model: Union[str, Language] = \"en_core_sci_sm\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Process text to extract abbreviations using spaCy with scispacy.\n",
    "    Args:\n",
    "        text (str): The text to process for abbreviations\n",
    "        model (Union[str, Language]): SpaCy model name or loaded model.\n",
    "            Defaults to \"en_core_sci_sm\".\n",
    "    Returns:\n",
    "        List[Dict]: List of abbreviation dictionaries, each containing:\n",
    "            - short_text: The abbreviated form\n",
    "            - short_start: Start position of short form\n",
    "            - short_end: End position of short form\n",
    "            - long_text: The expanded form\n",
    "            - long_start: Start position of long form\n",
    "            - long_end: End position of long form\n",
    "    Example:\n",
    "        >>> text = \"Magnetic resonance imaging (MRI) is a medical imaging technique\"\n",
    "        >>> abbrevs = load_abbreviations(text)\n",
    "        >>> print(abbrevs[0]['short_text'])  # 'MRI'\n",
    "        >>> print(abbrevs[0]['long_text'])   # 'Magnetic resonance imaging'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(model, str):\n",
    "            try:\n",
    "                nlp = spacy.load(model, disable=[\"parser\", \"ner\"])\n",
    "            except OSError:\n",
    "                print(f\"Downloading {model} model...\")\n",
    "                spacy.cli.download(model)\n",
    "                nlp = spacy.load(model, disable=[\"parser\", \"ner\"])\n",
    "        else:\n",
    "            nlp = model\n",
    "\n",
    "        # Add abbreviation detector if not present\n",
    "        if \"abbreviation_detector\" not in nlp.pipe_names:\n",
    "            try:\n",
    "                import scispacy.abbreviation  # noqa: F401\n",
    "\n",
    "                nlp.add_pipe(\"abbreviation_detector\")\n",
    "            except ImportError as e:\n",
    "                raise ImportError(\n",
    "                    f\"scispacy is required for abbreviation detection: {e}\"\n",
    "                )\n",
    "\n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        abbreviations = []\n",
    "\n",
    "        # Extract and serialize abbreviations\n",
    "        for short in doc._.abbreviations:\n",
    "            long = short._.long_form\n",
    "            abbreviations.append(\n",
    "                {\n",
    "                    \"short_text\": short.text,\n",
    "                    \"short_start\": short.start_char,\n",
    "                    \"short_end\": short.end_char,\n",
    "                    \"long_text\": long.text,\n",
    "                    \"long_start\": long.start_char,\n",
    "                    \"long_end\": long.end_char,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return abbreviations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error processing abbreviations: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def resolve_abbreviations(\n",
    "    target: str,\n",
    "    abbreviations: List[Dict],\n",
    ") -> str:\n",
    "    \"\"\"Resolve abbreviations in target text using a list of known abbreviations.\n",
    "    Finds and expands all abbreviations in the text, but only processes each unique\n",
    "    abbreviation once (using its first occurrence).\n",
    "    Args:\n",
    "        target (str): The text string that may contain abbreviations\n",
    "        abbreviations (List[Dict]): List of abbreviation dictionaries from load_abbreviations()\n",
    "        remove_parenthetical (bool): Removes parenthetical abbreviations from the text.\n",
    "            Defaults to True.\n",
    "    Returns:\n",
    "        str: Text with abbreviations expanded to their full forms\n",
    "    Example:\n",
    "        >>> text = \"The MRI showed abnormal MRI results. Both EEG and MRI indicated...\"\n",
    "        >>> abbrevs = load_abbreviations(\n",
    "        ...     \"Magnetic resonance imaging (MRI) and electroencephalogram (EEG)...\"\n",
    "        ... )\n",
    "        >>> expanded = resolve_abbreviations(text, abbrevs)\n",
    "        >>> print(expanded)\n",
    "        >>> # Result: First MRI expanded, EEG expanded, subsequent MRIs unchanged\n",
    "    \"\"\"\n",
    "    if not target or not abbreviations:\n",
    "        return target\n",
    "\n",
    "    # Track which abbreviations we've already processed\n",
    "    processed_abbrevs = set()\n",
    "    result = target\n",
    "\n",
    "    # Find all abbreviations that appear in the target\n",
    "    matching_abbrevs = [\n",
    "        abrv\n",
    "        for abrv in abbreviations\n",
    "        if abrv[\"short_text\"] in target and abrv[\"short_text\"] not in processed_abbrevs\n",
    "    ]\n",
    "\n",
    "    # Process each unique abbreviation (first occurrence only)\n",
    "    for abrv in matching_abbrevs:\n",
    "        short_form = abrv[\"short_text\"]\n",
    "        if short_form not in processed_abbrevs:\n",
    "            result = result.replace(short_form, abrv[\"long_text\"])\n",
    "            processed_abbrevs.add(short_form)\n",
    "\n",
    "    return result\n",
    "\n",
    "def find_and_remove_definitions(s, abbreviations):\n",
    "    \"\"\"\n",
    "    Find and remove definitions from the input string.\n",
    "    Args:\n",
    "        s (str): The input string to process.\n",
    "        abbreviations (List[Dict]): List of abbreviation dictionaries.\n",
    "    Returns:\n",
    "        str: The modified string with definitions removed.\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    modified_words = []\n",
    "\n",
    "    # Iterate through the words\n",
    "    for i, word in enumerate(words):\n",
    "        # Assume the word will be kept unless it's a definition to be removed\n",
    "        is_definition_to_remove = False\n",
    "\n",
    "        # Check if the word starts with '(' and ends with ')'\n",
    "        if word.startswith('(') and word.endswith(')'):\n",
    "            clause = word[1:-1]\n",
    "\n",
    "            # Check if the clause is a known abbreviation\n",
    "            for abbreviation in abbreviations:\n",
    "                if abbreviation[\"short_text\"] == clause:\n",
    "                    is_definition_to_remove = True\n",
    "                    break\n",
    "\n",
    "            # Also check if the clause is a recently defined abbreviation\n",
    "            clause_len = len(clause)\n",
    "            if i >= clause_len:\n",
    "                if not clause:  # Handles the case of \"()\"\n",
    "                    is_definition_to_remove = True\n",
    "                else:\n",
    "                    # Form the potential abbreviation from the first letters of preceding words\n",
    "                    # s.split() ensures words in `words` are non-empty, so `prev_word[0]` is safe.\n",
    "                    preceding_abbr = \"\".join(prev_word[0] for prev_word in words[i-clause_len : i])\n",
    "                    if preceding_abbr.lower() == clause.lower():\n",
    "                        is_definition_to_remove = True\n",
    "\n",
    "        if not is_definition_to_remove:\n",
    "            modified_words.append(word)\n",
    "\n",
    "    # Join the modified words back into a single string\n",
    "    return ' '.join(modified_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_folder(fol_path: Path, base_projects_folder: Path):\n",
    "    \"\"\"\n",
    "    Processes a single folder (fol_path) to extract and transform data.\n",
    "    base_projects_folder is the equivalent of the global 'projects_folder'.\n",
    "    Returns a list of processed group dictionaries, or an empty list on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"Processing {fol_path}\") # Optional: for debugging\n",
    "        processed_groups_for_this_fol = []\n",
    "        results_json_file = fol_path / 'results.json'\n",
    "        info_json_file = fol_path / 'info.json'\n",
    "\n",
    "        if not results_json_file.exists():\n",
    "            # print(f\"Skipping {fol_path}: results.json not found.\")\n",
    "            return processed_groups_for_this_fol\n",
    "        if not info_json_file.exists():\n",
    "            # print(f\"Skipping {fol_path}: info.json not found.\")\n",
    "            return processed_groups_for_this_fol\n",
    "\n",
    "        with open(results_json_file, 'r') as f:\n",
    "            results_content = json.load(f) # Renamed from 'results'\n",
    "\n",
    "        with open(info_json_file, 'r') as f:\n",
    "            info = json.load(f)\n",
    "\n",
    "        # --- Input path resolution (carefully matching original logic intent) ---\n",
    "        input_path_from_info_str = list(info['inputs'].keys())[0]\n",
    "        input_path_from_info = Path(input_path_from_info_str)\n",
    "        final_input_path = None\n",
    "\n",
    "        if not input_path_from_info.is_absolute():\n",
    "            if not input_path_from_info.parts: # Empty path string (e.g., Path(\"\"))\n",
    "                print(f\"Warning: Empty input path in info.json for {fol_path}\")\n",
    "                return processed_groups_for_this_fol\n",
    "\n",
    "            # Original logic: if relative, strip the first component of the path\n",
    "            # and make it relative to base_projects_folder.\n",
    "            if len(input_path_from_info.parts) > 1:\n",
    "                remaining_parts = input_path_from_info.parts[1:]\n",
    "                relative_tail = Path(*remaining_parts)\n",
    "                final_input_path = base_projects_folder / relative_tail\n",
    "            else: # Single component relative path (e.g., \"file.txt\")\n",
    "                  remaining_parts = input_path_from_info.parts[1:]\n",
    "                  if remaining_parts: # If \"dir/file.txt\", remaining_parts is (\"file.txt\",)\n",
    "                      relative_tail = Path(*remaining_parts)\n",
    "                      final_input_path = base_projects_folder / relative_tail\n",
    "                  elif input_path_from_info.name: # If \"file.txt\", remaining_parts is empty, but name exists\n",
    "                      # This is a common interpretation: projects_folder / filename\n",
    "                      final_input_path = base_projects_folder / input_path_from_info.name\n",
    "                  else: # Path was like \"somedir/\" or just \".\" and parts[1:] was problematic\n",
    "                      print(f\"Warning: Could not resolve relative input path '{input_path_from_info_str}' robustly for {fol_path} using parts[1:] logic. Falling back.\")\n",
    "                      final_input_path = base_projects_folder / input_path_from_info # General fallback\n",
    "        else:\n",
    "            final_input_path = input_path_from_info # It's absolute\n",
    "\n",
    "        if final_input_path is None or not final_input_path.exists() or final_input_path.is_dir():\n",
    "            # print(f\"Skipping {fol_path}: Input file {final_input_path} not found or is a directory.\")\n",
    "            return processed_groups_for_this_fol\n",
    "        # --- End Input path resolution ---\n",
    "\n",
    "        with open(final_input_path, 'r', encoding='utf-8') as text_file: # Added encoding\n",
    "            text = text_file.read()\n",
    "\n",
    "        abbreviations = load_abbreviations(text)\n",
    "\n",
    "        for group in results_content.get('groups', []):\n",
    "            # Iterate over a list of (key, value) items if modifying the dict by adding keys.\n",
    "            # In this case, new keys are added (_resolved), so this is safer.\n",
    "            for key, value in list(group.items()):\n",
    "                if isinstance(value, str):\n",
    "                    resolved_key_name = f\"{key}_resolved\"\n",
    "                    processed_value = find_and_remove_definitions(value, abbreviations)\n",
    "                    processed_value = resolve_abbreviations(processed_value, abbreviations)\n",
    "                    group[resolved_key_name] = normalize_string(processed_value)\n",
    "\n",
    "        # --- Identifiers.json path resolution (based on *resolved* final_input_path) ---\n",
    "        # Original: identifiers_json = input.parents[2] / 'identifiers.json'\n",
    "        # 'input' in original context was the resolved path. So, use final_input_path.\n",
    "        if len(final_input_path.parents) > 2: # Ensures parents[0,1,2] exist\n",
    "            identifiers_json_path = final_input_path.parents[2] / 'identifiers.json'\n",
    "            if identifiers_json_path.exists():\n",
    "                with open(identifiers_json_path, 'r', encoding='utf-8') as f: # Added encoding\n",
    "                    identifiers_data = json.load(f)\n",
    "                pmid = identifiers_data.get('pmid')\n",
    "                if pmid is not None:\n",
    "                    for group in results_content.get('groups', []):\n",
    "                        group['pmid'] = pmid\n",
    "            # else:\n",
    "                # print(f\"Identifiers file {identifiers_json_path} not found for {fol_path}\")\n",
    "        # else:\n",
    "            # print(f\"Input path {final_input_path} for {fol_path} does not have enough parent directories for identifiers.json.\")\n",
    "        # --- End Identifiers.json path resolution ---\n",
    "\n",
    "        # Append a copy of each modified group\n",
    "        for group in results_content.get('groups', []):\n",
    "            processed_groups_for_this_fol.append(group.copy()) # Append copies\n",
    "\n",
    "        return processed_groups_for_this_fol\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {fol_path}: {e}\")\n",
    "        # import traceback # For more detailed debugging\n",
    "        # traceback.print_exc()\n",
    "        return [] # Return empty list on error to not break the whole process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results_aggregated = []\n",
    "\n",
    "# # Ensure we are only trying to process directories within data_source\n",
    "# folders_to_process = [fol for fol in data_source.iterdir() if fol.is_dir()]\n",
    "\n",
    "# # Use ProcessPoolExecutor for parallel processing\n",
    "# # max_workers=None will use the number of CPUs on the machine\n",
    "# with ProcessPoolExecutor(max_workers=None) as executor:\n",
    "#     # Use functools.partial to pass the 'projects_folder' argument to the worker function\n",
    "#     # This keeps the worker function signature clean for executor.map\n",
    "#     worker_task = partial(process_single_folder, base_projects_folder=projects_folder)\n",
    "    \n",
    "#     # executor.map applies worker_task to each item in folders_to_process\n",
    "#     # It returns an iterator, so convert to list to ensure all tasks complete and results are gathered\n",
    "#     list_of_group_lists = list(executor.map(worker_task, folders_to_process))\n",
    "\n",
    "# # Flatten the list of lists into a single list of group dictionaries\n",
    "# for group_list_from_folder in list_of_group_lists:\n",
    "#     if group_list_from_folder: # Check if the list is not empty (e.g., due to an error in worker)\n",
    "#         all_results_aggregated.extend(group_list_from_folder)\n",
    "\n",
    "# # Create a DataFrame from the aggregated list of dictionaries\n",
    "# results_df = pd.DataFrame(all_results_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group_name\n",
       "healthy      23999\n",
       "patients     16417\n",
       "relatives        3\n",
       "controls         2\n",
       "siblings         1\n",
       "children         1\n",
       "adults           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.group_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis_resolved\n",
       "Schizophrenia Spectrum Disorder                                                                                                          19\n",
       "Attention Deficit/Hyperactivity Disorder                                                                                                 19\n",
       "Healthy Comparison Subjects                                                                                                              19\n",
       "Huntington'S Disease                                                                                                                     19\n",
       "Essential Tremor                                                                                                                         19\n",
       "                                                                                                                                         ..\n",
       "Bipolar I Disorder, Bipolar Ii Disorder, Or Bipolar Disorder Not Otherwise Specified                                                      1\n",
       "Subcortical Ischemic Vascular Disease-Vascular Dementia (Sivascular Dementia-Vascular Dementia)                                           1\n",
       "Subcortical Ischemic Vascular Disease-Vascular Cognitive Impairment No Dementia (Sivascular Dementia-Vascular Cognitive Impairmentnd)     1\n",
       "Subcortical Ischemic Vascular Disease-Normal Cognitive Impairment (Sivascular Dementia-Normal Controli)                                   1\n",
       "Visual Form Agnosia, Prosopagnosia                                                                                                        1\n",
       "Name: count, Length: 7594, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.diagnosis_resolved.value_counts()[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONVOC normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from collections import defaultdict\n",
    "normalized = json.load(open('onvoc-normalized.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = defaultdict(list)\n",
    "\n",
    "for k, v in normalized.items():\n",
    "    mappings[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2182"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mappings['None of the above / Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini mappings for None/ Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = json.load(open('onvoc-gemini-mappings.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      NaN\n",
       "1                                                      NaN\n",
       "2        Tinnitus:Including Idiopathic Tinnitus (variou...\n",
       "3                                                      NaN\n",
       "4                                                      NaN\n",
       "                               ...                        \n",
       "40419                                                  NaN\n",
       "40420                                                  NaN\n",
       "40421                                                  NaN\n",
       "40422                                                  NaN\n",
       "40423                                                  NaN\n",
       "Name: diagnosis_resolved, Length: 40424, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['diagnosis_resolved'].map(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {k: v.split(':')[0] for k, v in mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['onvoc_diagnosis_gemini+'] = results_df['diagnosis_resolved'].map(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onvoc_diagnosis_gemini+\n",
       "None of the above / Other                                             729\n",
       "Aphasia and Related Communication Disorders                           172\n",
       "Tinnitus                                                              155\n",
       "Nicotine/Tobacco                                                      139\n",
       "Chromosomal Abnormalities & Associated Syndromes                      104\n",
       "Dissociative Disorders                                                 94\n",
       "Amyotrophic Lateral Sclerosis (ALS) / Motor Neuron Disease             93\n",
       "Dystonias                                                              90\n",
       "HIV-Related Conditions                                                 86\n",
       "Strabismus & Amblyopia                                                 71\n",
       "Trichotillomania, Hair Pulling & Skin Picking Disorders                69\n",
       "Burns & Electrical Injury                                              68\n",
       "Huntington's Disease (HD)                                              66\n",
       "Depressive Disorders and Symptoms                                      65\n",
       "Impulse Control Disorders & Impulsivity                                65\n",
       "Cerebrovascular Conditions (Stroke, Lesions, Small Vessel Disease)     64\n",
       "Other Neurodevelopmental & Genetic Syndromes                           64\n",
       "Ataxias                                                                60\n",
       "Menopause & Hormonal Conditions (Female)                               60\n",
       "Gastrointestinal Disorders                                             56\n",
       "Multiple Sclerosis (MS) & Related Demyelinating Diseases               56\n",
       "Behavioral Addictions                                                  56\n",
       "Other Visual Conditions                                                54\n",
       "Amputation & Limb Deficiencies                                         52\n",
       "Cannabis                                                               51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.onvoc_diagnosis == 'None of the above / Other'][\"onvoc_diagnosis_gemini+\"].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('pd_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
